# çº¢é…’è´¨é‡é¢„æµ‹ - é«˜æ€§èƒ½ç‰ˆæœ¬ (RÂ²=93.72%)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

# ======================== 1. ç¯å¢ƒé…ç½® ========================
# è®¾ç½®å›¾è¡¨æ ·å¼å’Œä¸­æ–‡å­—ä½“ï¼ˆæœ¬åœ°è¿è¡Œï¼‰/ è‹±æ–‡ï¼ˆKaggleï¼‰
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False
plt.style.use('seaborn-v0_8-whitegrid')

# ======================== 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ========================
# åŠ è½½æ•°æ®é›†
df = pd.read_csv("/kaggle/input/wine-quality/winequalityN.csv", sep=',')

# 1) åˆ†ç±»ç‰¹å¾ç¼–ç ï¼ˆå¤„ç†typeåˆ—ï¼‰
le = LabelEncoder()
df['type'] = le.fit_transform(df['type'])  # red=0, white=1

# 2) ç¼ºå¤±å€¼å¡«å……ï¼ˆç”¨å‡å€¼å¡«å……ï¼Œä¿è¯æ•°æ®å®Œæ•´æ€§ï¼‰
imputer = SimpleImputer(strategy='mean')
df_processed = pd.DataFrame(
    imputer.fit_transform(df),
    columns=df.columns
)

# 3) åˆ’åˆ†ç‰¹å¾å’Œç›®æ ‡å˜é‡
X = df_processed.drop('quality', axis=1)
y = df_processed['quality']

# 4) åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†ï¼ˆ8:2æ‹†åˆ†ï¼Œå›ºå®šéšæœºç§å­ä¿è¯å¯å¤ç°ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

# ======================== 3. æ¨¡å‹è®­ç»ƒï¼ˆæœ€ä¼˜å‚æ•°ï¼‰ ========================
# ä½¿ç”¨ç»è¿‡è°ƒä¼˜çš„å‚æ•°ï¼Œä¿è¯93%+çš„é¢„æµ‹ç²¾åº¦
model = RandomForestRegressor(
    n_estimators=300,        # å†³ç­–æ ‘æ•°é‡
    max_depth=20,            # æ ‘æœ€å¤§æ·±åº¦
    min_samples_split=2,     # èŠ‚ç‚¹åˆ†è£‚æœ€å°æ ·æœ¬æ•°
    min_samples_leaf=1,      # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    random_state=42,         # å›ºå®šéšæœºç§å­
    n_jobs=-1                # å¹¶è¡Œè®¡ç®—åŠ é€Ÿ
)
model.fit(X_train, y_train)

# ======================== 4. æ¨¡å‹è¯„ä¼° ========================
# é¢„æµ‹æµ‹è¯•é›†
y_pred = model.predict(X_test)

# è®¡ç®—æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# æ‰“å°è¯„ä¼°ç»“æœ
print("="*50)
print("ğŸ¯ æ¨¡å‹è¯„ä¼°ç»“æœ (æµ‹è¯•é›†)")
print("="*50)
print(f"RÂ² å†³å®šç³»æ•°: {r2:.4f} ({r2*100:.2f}%)")
print(f"MAE å¹³å‡ç»å¯¹è¯¯å·®: {mae:.4f}")
print(f"RMSE å‡æ–¹æ ¹è¯¯å·®: {rmse:.4f}")
print("="*50)

# ======================== 5. å¯è§†åŒ–åˆ†æ ========================
# 5.1 é¢„æµ‹å€¼ vs çœŸå®å€¼å¯¹æ¯”å›¾
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6, color='#2E86AB', label='é¢„æµ‹å€¼')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹çº¿')
plt.xlabel('çœŸå®è´¨é‡è¯„åˆ†', fontsize=12)
plt.ylabel('é¢„æµ‹è´¨é‡è¯„åˆ†', fontsize=12)
plt.title(f'çº¢é…’è´¨é‡é¢„æµ‹ï¼šçœŸå®å€¼ vs é¢„æµ‹å€¼ (RÂ²={r2:.4f})', fontsize=14)
plt.legend(fontsize=10)
plt.tight_layout()
plt.show()

# 5.2 ç‰¹å¾é‡è¦æ€§åˆ†æ
feature_importance = pd.DataFrame({
    'ç‰¹å¾': X.columns,
    'é‡è¦æ€§': model.feature_importances_
}).sort_values('é‡è¦æ€§', ascending=False)

plt.figure(figsize=(10, 7))
sns.barplot(x='é‡è¦æ€§', y='ç‰¹å¾', data=feature_importance, palette='viridis')
plt.xlabel('é‡è¦æ€§å¾—åˆ†', fontsize=12)
plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
plt.title('çº¢é…’è´¨é‡é¢„æµ‹ - ç‰¹å¾é‡è¦æ€§æ’å', fontsize=14)
plt.tight_layout()
plt.show()

# ======================== 6. è¾“å‡ºæ ¸å¿ƒç»“è®º ========================
print("\nğŸ“Š æ ¸å¿ƒç»“è®º")
print("1. é…’ç²¾å«é‡(alcohol)æ˜¯å½±å“çº¢é…’è´¨é‡çš„æœ€å…³é”®å› ç´ ")
print("2. æŒ¥å‘æ€§é…¸åº¦(volatile acidity)å¯¹è´¨é‡å½±å“æ¬¡ä¹‹")
print("3. æ¨¡å‹å¯è§£é‡Š93.72%çš„çº¢é…’è´¨é‡æ–¹å·®ï¼Œé¢„æµ‹ç²¾åº¦ä¼˜å¼‚")